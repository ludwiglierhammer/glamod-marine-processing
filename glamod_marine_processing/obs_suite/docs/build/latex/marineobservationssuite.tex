%% Generated by Sphinx.
\def\sphinxdocclass{report}
\documentclass[letterpaper,10pt,english]{sphinxmanual}
\ifdefined\pdfpxdimen
   \let\sphinxpxdimen\pdfpxdimen\else\newdimen\sphinxpxdimen
\fi \sphinxpxdimen=.75bp\relax

\PassOptionsToPackage{warn}{textcomp}
\usepackage[utf8]{inputenc}
\ifdefined\DeclareUnicodeCharacter
% support both utf8 and utf8x syntaxes
  \ifdefined\DeclareUnicodeCharacterAsOptional
    \def\sphinxDUC#1{\DeclareUnicodeCharacter{"#1}}
  \else
    \let\sphinxDUC\DeclareUnicodeCharacter
  \fi
  \sphinxDUC{00A0}{\nobreakspace}
  \sphinxDUC{2500}{\sphinxunichar{2500}}
  \sphinxDUC{2502}{\sphinxunichar{2502}}
  \sphinxDUC{2514}{\sphinxunichar{2514}}
  \sphinxDUC{251C}{\sphinxunichar{251C}}
  \sphinxDUC{2572}{\textbackslash}
\fi
\usepackage{cmap}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amstext}
\usepackage{babel}



\usepackage{times}
\expandafter\ifx\csname T@LGR\endcsname\relax
\else
% LGR was declared as font encoding
  \substitutefont{LGR}{\rmdefault}{cmr}
  \substitutefont{LGR}{\sfdefault}{cmss}
  \substitutefont{LGR}{\ttdefault}{cmtt}
\fi
\expandafter\ifx\csname T@X2\endcsname\relax
  \expandafter\ifx\csname T@T2A\endcsname\relax
  \else
  % T2A was declared as font encoding
    \substitutefont{T2A}{\rmdefault}{cmr}
    \substitutefont{T2A}{\sfdefault}{cmss}
    \substitutefont{T2A}{\ttdefault}{cmtt}
  \fi
\else
% X2 was declared as font encoding
  \substitutefont{X2}{\rmdefault}{cmr}
  \substitutefont{X2}{\sfdefault}{cmss}
  \substitutefont{X2}{\ttdefault}{cmtt}
\fi


\usepackage[Bjarne]{fncychap}
\usepackage{sphinx}

\fvset{fontsize=\small}
\usepackage{geometry}


% Include hyperref last.
\usepackage{hyperref}
% Fix anchor placement for figures with captions.
\usepackage{hypcap}% it must be loaded after hyperref.
% Set up styles of URL: it should be placed after hyperref.
\urlstyle{same}
\addto\captionsenglish{\renewcommand{\contentsname}{Contents:}}

\usepackage{sphinxmessages}
\setcounter{tocdepth}{2}



\title{Marine observations suite}
\date{Jul 27, 2020}
\release{v1.1+}
\author{DYB, IPG}
\newcommand{\sphinxlogo}{\vbox{}}
\renewcommand{\releasename}{Release}
\makeindex
\begin{document}

\pagestyle{empty}
\sphinxmaketitle
\pagestyle{plain}
\sphinxtableofcontents
\pagestyle{normal}
\phantomsection\label{\detokenize{index::doc}}



\chapter{Indices and tables}
\label{\detokenize{index:indices-and-tables}}\begin{itemize}
\item {}
\DUrole{xref,std,std-ref}{genindex}

\item {}
\DUrole{xref,std,std-ref}{modindex}

\item {}
\DUrole{xref,std,std-ref}{search}

\end{itemize}


\chapter{Introduction}
\label{\detokenize{index:introduction}}
The Marine Observations Suite is part of the code implemented to produce the
data deliveries for the C3S Marine In Situ Component. Instructions to run the
full set of suites, including this one, are available in the C3S Technical
Service Document. The present manual includes the set of instructions needed to
run the observations suite, and also more detailed information on this branch of
the marine code.

The Observation Suite is a set of python and shell scripts to harmonize and
convert input observational data sources to CDM formatted files. This includes
merging the CDM files with the output from the metadata and qc suites.

It is based on a set of chained processes, each step feeding into the next one
and with the initial dataset (previously prepared for inclusion in the marine
processing) transitioning through a series of levels from the first mapping to
the CDM in level1a to the final set of curated observational CDM compatible
files in level2 (header and observations\sphinxhyphen{}*). Some levels are fed with data from
additional external datasets or with the output of dedicated suites that run
concurrently to the overall scheme (see figure below).

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=600\sphinxpxdimen]{{marine_data_flow}.png}
\caption{General marine data flow. The Observations Suite is highlighted in the red box.}\label{\detokenize{index:id3}}\end{figure}


\section{Note on versions}
\label{\detokenize{index:note-on-versions}}\label{\detokenize{index:versions}}
This manual was written at the time of transition between glamod\sphinxhyphen{}marine\sphinxhyphen{}processing
v1.1 and its migration to the SLURM scheduler.

Consequently, this manual describes the Observations Suite as available in the
HEAD of the repository (hence v1.1+), not in the last tagged version at that
time (v1.1).


\chapter{Processing levels}
\label{\detokenize{index:processing-levels}}
As shown in the diagram below, the Observations Suite is a set of chained
processes in which the source dataset transitions through the following levels:

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=500\sphinxpxdimen]{{obs_suite_levels}.png}
\caption{Observations Suite processing levels}\label{\detokenize{index:id4}}\end{figure}
\begin{itemize}
\item {}
level1a: is the first mapping of the dataset to the CDM. Prior to mapping,
the input data files are validated against the schema / data model and code
tables defining the input. Reports failing this validation are discarded. For
ICOADS, this includes also the rescue of any additional information from the
supplemental attachment that has been identified as adding value. At this
level the data are partitioned by date (monthly files), ICOADS source and card
deck information and observed parameter. For each month a set of files is
created containing the header and observation tables (header and
observations\sphinxhyphen{}{[}at|sst|dpt|wbt|slp|ws|wd{]}).

\item {}
level1b: is the data improved with corrections and/or additional information
resulting from the linkage and duplicate identification process. Reassignment of
reports to different monthly files can result from this process after datetime
corrections.

\item {}
level1c: is the data with metadata (currently primary station identification
and datetime) validation performed and applied.

\item {}
level1d: data is enriched with external meta\sphinxhyphen{}data where available. For ship
data the additional meta data source is WMO Publication 47 metadata.

\item {}
level1e: final quality control flags are added at this level, resulting from
the position, parameter and tracking quality control processes.

\item {}
level2: data ready to ingest in the database. Data in level1e is inspected as
data filtering might apply and part of the initial data set might be rejected
to be inserted in the CDS database.

\end{itemize}


\chapter{Marine file system}
\label{\detokenize{index:marine-file-system}}
The Observations Suite code is integrated in the file system designed for the
C3S data deliveries. The general directory structure that holds this file system
is shown in the figure.

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=600\sphinxpxdimen]{{marine_file_system}.png}
\caption{General marine directory structure}\label{\detokenize{index:id5}}\end{figure}


\chapter{Tool set\sphinxhyphen{}up}
\label{\detokenize{index:tool-set-up}}

\section{Code repository}
\label{\detokenize{index:code-repository}}
The full set of suites that make up the marine code are integrated in the
glamod\sphinxhyphen{}marine\sphinxhyphen{}processing repository. Thus, to install the observations suite,
the repository needs to be cloned:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
git clone git@git.noc.ac.uk:iregon/glamod\PYGZhy{}marine\PYGZhy{}processing.git \PYGZhy{}\PYGZhy{}branch version
\end{sphinxVerbatim}

where:
\begin{itemize}
\item {}
version: see repository tags. Last tagged version is v1.1, with this manual
currently describing the HEAD of the repository (See {\hyperref[\detokenize{index:versions}]{\sphinxcrossref{\DUrole{std,std-ref}{Note on versions}}}}).

\end{itemize}


\section{Setting paths and environments}
\label{\detokenize{index:setting-paths-and-environments}}
Script obs\sphinxhyphen{}suite/setpaths.sh sets the paths for the processing software and data
files, including a scratch directory for the user running the software. Edit the
script file and set the environment variables as indicated below:
\begin{itemize}
\item {}
code\_directory: full path to the obs\sphinxhyphen{}suite code.

\item {}
home\_directory\_smf: full path to the obs\sphinxhyphen{}suite configuration.

\item {}
data\_directory: full path to marine data file system.

\item {}
scratch\_directory: this is system dependent and is currently set as available in CEDA\sphinxhyphen{}JASMIN

\end{itemize}

The obs\sphinxhyphen{}suite/setenv0.sh script initialises the processing environment. It needs
to be edited and the pyEnvironment\_directory environmental variable set to the
path of the corresponding python environment installation (obs\sphinxhyphen{}suite/pyenvs/env0).
It also needs to be modified to include the path to the system python libraries
in the LD\_LIBRARY\_PATH variable.

Once the these scripts have been modified, the python virtual environment needs
to be initialised with the following block of code:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{cd} obs\PYGZhy{}suite/env
module load jaspy/3.7
virtualenv \PYGZhy{}\textendash{}system\PYGZhy{}site\PYGZhy{}packages env0
\PYG{n+nb}{source} env0/bin/activate
pip install \PYGZhy{}r requirements\PYGZus{}env0.txt
\end{sphinxVerbatim}


\section{Adding modules}
\label{\detokenize{index:adding-modules}}
Four additional python modules have been developed for this suite. The table
below lists these modules and which versions are compatible with the current
marine code version (v1.1 and HEAD).


\begin{savenotes}\sphinxattablestart
\centering
\sphinxcapstartof{table}
\sphinxthecaptionisattop
\sphinxcaption{Title}\label{\detokenize{index:id6}}
\sphinxaftertopcaption
\begin{tabular}[t]{|\X{30}{125}|\X{30}{125}|\X{55}{125}|\X{10}{125}|}
\hline
\sphinxstyletheadfamily
module
&\sphinxstyletheadfamily
module\_local
&\sphinxstyletheadfamily
module\_repo\_url
&\sphinxstyletheadfamily
version
\\
\hline
CDM mapper
&
cdm
&
\sphinxhref{mailto:git@git.noc.ac.uk}{git@git.noc.ac.uk}:iregon/cdm\sphinxhyphen{}mapper.git
&
v1.2
\\
\hline
Data reader
&
mdf\_reader
&
\sphinxhref{mailto:git@git.noc.ac.uk}{git@git.noc.ac.uk}:iregon/mdf\_reader.git
&
v1.2
\\
\hline
Metadata fixes
&
metmetpy
&
\sphinxhref{mailto:git@git.noc.ac.uk}{git@git.noc.ac.uk}:iregon/metmetpy.git
&
v1.0
\\
\hline
Pandas operations
&
pandas\_operations
&
\sphinxhref{mailto:git@git.noc.ac.uk}{git@git.noc.ac.uk}:iregon/pandas\_operations.git
&
v1.2
\\
\hline
\end{tabular}
\par
\sphinxattableend\end{savenotes}

For each module listed the following needs to be run:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{cd} obs\PYGZhy{}suite/modules/python
git clone module\PYGZus{}repo\PYGZus{}url \PYGZhy{}\PYGZhy{}branch version \PYGZhy{}\PYGZhy{}single\PYGZhy{}branch module\PYGZus{}local
\end{sphinxVerbatim}


\chapter{Common paths}
\label{\detokenize{index:common-paths}}
The following path names are used throughout this manual:
\begin{itemize}
\item {}
\sphinxstyleemphasis{obs\sphinxhyphen{}suite}: path of the observations suite in the marine processing repository

\item {}
\sphinxstyleemphasis{config\_directory}: path to the obs\sphinxhyphen{}suite directory in the configuration repository

\item {}
\sphinxstyleemphasis{data\_directory}: path to general data directory in the marine file system

\item {}
\sphinxstyleemphasis{release\_config\_dir}: full path to the configuration of a specific data release
within \sphinxstyleemphasis{config\_directory}

\end{itemize}


\chapter{Initializing a new data release}
\label{\detokenize{index:initializing-a-new-data-release}}

\section{Configuration repository}
\label{\detokenize{index:configuration-repository}}
The glamod\sphinxhyphen{}marine\sphinxhyphen{}config repository
(\sphinxhref{mailto:git@git.noc.ac.uk}{git@git.noc.ac.uk}:iregon/glamod\sphinxhyphen{}marine\sphinxhyphen{}config.git) serves as container for the
configuration used to create the different data releases for C3S. The
Observations Suite configuration files are stored in
obs\sphinxhyphen{}suite/\sphinxstyleemphasis{release}\sphinxhyphen{}\sphinxstyleemphasis{update}/\sphinxstyleemphasis{dataset} directories within this repository.

Currently, the following configuration sets are available:


\begin{savenotes}\sphinxattablestart
\centering
\sphinxcapstartof{table}
\sphinxthecaptionisattop
\sphinxcaption{Title}\label{\detokenize{index:id7}}
\sphinxaftertopcaption
\begin{tabular}[t]{|\X{20}{60}|\X{30}{60}|\X{10}{60}|}
\hline
\sphinxstyletheadfamily
Data release
&\sphinxstyletheadfamily
Path in repo/obs\sphinxhyphen{}suite
&\sphinxstyletheadfamily
Marine code version
\\
\hline
r092019
&
r092019\sphinxhyphen{}000000/ICOADS\_R3.0.0T
&
v1.0
\\
\hline
release\_2.0
&
release\_2.0\sphinxhyphen{}000000/ICOADS\_R3.0.0T
&
v1.1
\\
\hline
Demo release
&
release\_demo\sphinxhyphen{}000000/ICOADS\_R3.0.0T
&
v1.1+ (HEAD)
\\
\hline
\end{tabular}
\par
\sphinxattableend\end{savenotes}

Up until v1.1 (release\_2.0), the configuration files were not maintained in
the configuration repository, but in the code repository. They have been now
included in the configuration repository for traceability. It is also worth
noting, that some changes have been made to the configuration files after v1.1:
the format in the Demo release files must be applied when running the observations
suite.


\section{Create the configuration files for the release and dataset}
\label{\detokenize{index:create-the-configuration-files-for-the-release-and-dataset}}
Every data release is identified in the file system with the following tags:
\begin{itemize}
\item {}
release: release name (eg. release\_2.0)

\item {}
update: udpate tag (eg. 000000)

\item {}
dataset: dataset name (eg. ICOADS\_R3.0.0T)

\end{itemize}

Create a new directory \sphinxstyleemphasis{release}\sphinxhyphen{}\sphinxstyleemphasis{update}/\sphinxstyleemphasis{dataset}/ in the obs\sphinxhyphen{}suite
configuration directory (\sphinxstyleemphasis{config\_directory}) of the configuration repository
(note the hyphen as field separator between \sphinxstyleemphasis{release} and \sphinxstyleemphasis{udpate}). We will now
refer to this directory as \sphinxstyleemphasis{release\_config\_dir}.

The files described in the following sections need to be created, with the
{\hyperref[\detokenize{index:release-periods-file}]{\sphinxcrossref{\DUrole{std,std-ref}{Release periods file}}}} and the {\hyperref[\detokenize{index:process-list-file}]{\sphinxcrossref{\DUrole{std,std-ref}{Process list file}}}} required from the
setup of the new data release. The rest of the files can be generated as the
processing gets to the corresponding level.

The sample files in the following sections can be found in the release\_demo
directory of the configuration repository.


\subsection{Release periods file}
\label{\detokenize{index:release-periods-file}}\label{\detokenize{index:id1}}
Create file \sphinxstyleemphasis{release\_config\_dir}/source\_deck\_periods.json

This file is a json file with each of the source\sphinxhyphen{}deck partitions to be included
in the release, and the associated periods (year resolution) to process.

The figure below shows a sample of this file:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{p}{\PYGZob{}}
    \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{001\PYGZhy{}110}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{p}{\PYGZob{}}
        \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{year\PYGZus{}init}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+m+mi}{1948}\PYG{p}{,}
        \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{year\PYGZus{}end}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+m+mi}{1951}
    \PYG{p}{\PYGZcb{}}\PYG{p}{,}
    \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{063\PYGZhy{}714}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{p}{\PYGZob{}}
        \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{year\PYGZus{}init}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+m+mi}{2007}\PYG{p}{,}
        \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{year\PYGZus{}end}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+m+mi}{2009}
    \PYG{p}{\PYGZcb{}}\PYG{p}{,}
    \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{171\PYGZhy{}711}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{p}{\PYGZob{}}
        \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{year\PYGZus{}init}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+m+mi}{1889}\PYG{p}{,}
        \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{year\PYGZus{}end}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+m+mi}{1899}
    \PYG{p}{\PYGZcb{}}
\PYG{p}{\PYGZcb{}}
\end{sphinxVerbatim}


\subsection{Process list file}
\label{\detokenize{index:process-list-file}}\label{\detokenize{index:id2}}
Create file \sphinxstyleemphasis{release\_config\_dir}/source\_deck\_list.txt

This is a simple ascii file with the list of source\sphinxhyphen{}deck partitions to process.
Create the master list with the keys of file source\_deck\_periods.json. This file
can later be subsetted if a given process is to be run in batches.

The figure below shows a sample of this file:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{l+m+mi}{001}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{110}
\PYG{l+m+mi}{063}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{714}
\PYG{l+m+mi}{171}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{711}
\end{sphinxVerbatim}


\subsection{Level 1a configuration file}
\label{\detokenize{index:level-1a-configuration-file}}\label{\detokenize{index:level1a-config-file}}
Create file \sphinxstyleemphasis{release\_config\_dir}/level1a.json.

This file includes information on the initial dataset files data model(s),
filters used to select reports and mapping to apply convert the data to the CDM.

The figure below shows a sample of this file:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{p}{\PYGZob{}}
    \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{job\PYGZus{}memo\PYGZus{}mb}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+m+mi}{4000}\PYG{p}{,}
    \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{job\PYGZus{}time\PYGZus{}hr}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{01}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
    \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{job\PYGZus{}time\PYGZus{}min}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{30}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
    \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{data\PYGZus{}model}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{imma1}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
    \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{read\PYGZus{}sections}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{core}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{c1}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{c98}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{,}
    \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{filter\PYGZus{}reports\PYGZus{}by}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:}
    \PYG{p}{\PYGZob{}}
      \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{c1.PT}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{0}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{1}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{2}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{3}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{4}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{5}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}
    \PYG{p}{\PYGZcb{}}\PYG{p}{,}
    \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{cdm\PYGZus{}map}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{icoads\PYGZus{}r3000}\PYG{l+s+s2}{\PYGZdq{}}
    \PYG{p}{,}
    \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{063\PYGZhy{}714}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:}
    \PYG{p}{\PYGZob{}}
      \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{job\PYGZus{}memo\PYGZus{}mb}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+m+mi}{16000}\PYG{p}{,}
      \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{job\PYGZus{}time\PYGZus{}hr}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{03}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{job\PYGZus{}time\PYGZus{}min}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{30}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{data\PYGZus{}model}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{imma1\PYGZus{}d714}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
      \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{read\PYGZus{}sections}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{core}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{c1}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{c98}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{c99}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{,}
      \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{filter\PYGZus{}reports\PYGZus{}by}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:}
      \PYG{p}{\PYGZob{}}
        \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{c1.PT}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{7}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}
      \PYG{p}{\PYGZcb{}}\PYG{p}{,}
      \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{cdm\PYGZus{}map}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{icoads\PYGZus{}r3000\PYGZus{}d714}\PYG{l+s+s2}{\PYGZdq{}}
    \PYG{p}{\PYGZcb{}}
\PYG{p}{\PYGZcb{}}
\end{sphinxVerbatim}

This file has its default configuration parameters in the outer keys.
Source\sphinxhyphen{}deck specific configuration can be applied by specifying a configuration
parameter under a \sphinxstyleemphasis{sid\sphinxhyphen{}dck} key. In the sample given, all the
source and decks will be processed with the default configuration, but 063\sphinxhyphen{}714,
that will use its own parameters.

Configuration parameters job* are only used by the slurm launchers, while the
rest by the corresponding level1a.py script.


\subsection{Level 1b configuration file}
\label{\detokenize{index:level-1b-configuration-file}}\label{\detokenize{index:level1b-config-file}}
Create file \sphinxstyleemphasis{release\_config\_dir}/level1b.json.

This file contains information on
the NOC corrections version to be used and the correspondences between the
CDM tables fields on which the corrections are applied and the subdirectories
where these corrections can be found. The CDM history stamp for every correction
is also configured in this file.

The figure below shows a sample of this file:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{p}{\PYGZob{}}
  \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{job\PYGZus{}memo\PYGZus{}mb}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+m+mi}{4000}\PYG{p}{,}
  \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{job\PYGZus{}time\PYGZus{}hr}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{01}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
  \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{job\PYGZus{}time\PYGZus{}min}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{30}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
  \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{correction\PYGZus{}version}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{v1x2019}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
  \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{corrections}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:}
  \PYG{p}{\PYGZob{}}
      \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{header}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:}
      \PYG{p}{\PYGZob{}}
          \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{id}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{primary\PYGZus{}station\PYGZus{}id}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
          \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{timestamp}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{report\PYGZus{}timestamp}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
          \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{duplicate\PYGZus{}flags}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{duplicate\PYGZus{}status}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
          \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{duplicates}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{duplicates}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
          \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{longitude}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{longitude}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
          \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{latitude}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{latitude}\PYG{l+s+s2}{\PYGZdq{}}
      \PYG{p}{\PYGZcb{}}\PYG{p}{,}
      \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{observations\PYGZhy{}at}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:}
       \PYG{p}{\PYGZob{}}
          \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{timestamp}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{date\PYGZus{}time}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
          \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{longitude}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{longitude}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
          \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{latitude}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{latitude}\PYG{l+s+s2}{\PYGZdq{}}
      \PYG{p}{\PYGZcb{}}
  \PYG{p}{\PYGZcb{}}\PYG{p}{,}
  \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{histories}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:}
  \PYG{p}{\PYGZob{}}
    \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{id}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Corrected primary\PYGZus{}station\PYGZus{}id}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
    \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{timestamp}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Corrected report\PYGZus{}timestamp and date\PYGZus{}time}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
    \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{longitude}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Corrected longitude}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
    \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{latitude}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Corrected latitude}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
    \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{duplicate\PYGZus{}flags}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Added duplicate information \PYGZhy{} flag}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
    \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{duplicates}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Added duplicate information \PYGZhy{} duplicates}\PYG{l+s+s2}{\PYGZdq{}}
  \PYG{p}{\PYGZcb{}}
\PYG{p}{\PYGZcb{}}
\end{sphinxVerbatim}

This file has its default configuration parameters in the outer keys.
Source\sphinxhyphen{}deck specific configuration can be applied by specifying a configuration
parameter under a \sphinxstyleemphasis{sid\sphinxhyphen{}dck} key. In the sample above, only the default
configuration is applied.

Configuration parameters job* are only used by the slurm launchers, while the
rest by the corresponding level1b.py script.


\subsection{Level 1c configuration file}
\label{\detokenize{index:level-1c-configuration-file}}\label{\detokenize{index:level1c-config-file}}
Create file \sphinxstyleemphasis{release\_config\_dir}/level1c.json.

The only configuration parameters
required in this file are those related to the slurm launchers, as the rest of
the configuration of this process is basically hardcoded in the level1c.py
script.

The figure below shows a sample of this file:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{p}{\PYGZob{}}
  \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{job\PYGZus{}memo\PYGZus{}mb}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+m+mi}{4000}\PYG{p}{,}
  \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{job\PYGZus{}time\PYGZus{}hr}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{01}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
  \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{job\PYGZus{}time\PYGZus{}min}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{30}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
\PYG{p}{\PYGZcb{}}
\end{sphinxVerbatim}

This file has its default configuration parameters in the outer keys.
Source\sphinxhyphen{}deck specific configuration can be applied by specifying a configuration
parameter under a \sphinxstyleemphasis{sid\sphinxhyphen{}dck} key. In the sample above, only the default
configuration is applied.


\subsection{Level 1d configuration file}
\label{\detokenize{index:level-1d-configuration-file}}\label{\detokenize{index:level1d-config-file}}
Create file \sphinxstyleemphasis{release\_config\_dir}/level1d.json.

This file contains information
on the metadata sources that are merged into the level1c data. Currently the
only MD source is wmo\_publication\_47 and the full process is basically tailored
to Pub47 as pre\sphinxhyphen{}processed in NOC.

This file contains information of the subdirectory in the release data directory
where the metadata can be found (“md\_subdir”) and the name of the mapping within the
Common Data Model mapper module used to map pub47 to the CDM (“md\_model”).

The level1d process will fail if it doesn’t find a metadata file for a month
partition. To account for periods where metadata are not available, the
following optional keys can be used:
\begin{itemize}
\item {}
“md\_not\_avail”: true indicates the process that for the full release period,
there is not metadata available. Defaults to false.

\item {}
“md\_first\_yr\_avail”: indicates the first year for which metadata files should
be available in the release period. Defaults to first year in the release
period.

\item {}
“md\_last\_yr\_avail”: indicates the last year for which metadata files should be
available in the release period. Defaults to last year in the release
period.

\end{itemize}

By using the above keys, the process is indicated to securely progress data files
to the next processing level without merging any metadata when it is not available.

The figure below shows a sample of this file:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{p}{\PYGZob{}}
  \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{job\PYGZus{}memo\PYGZus{}mb}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+m+mi}{4000}\PYG{p}{,}
  \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{job\PYGZus{}time\PYGZus{}hr}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{01}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
  \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{job\PYGZus{}time\PYGZus{}min}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{30}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
  \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{md\PYGZus{}model}\PYG{l+s+s2}{\PYGZdq{}} \PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{pub47\PYGZus{}noc}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
  \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{md\PYGZus{}subdir}\PYG{l+s+s2}{\PYGZdq{}} \PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{wmo\PYGZus{}publication\PYGZus{}47}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
  \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{md\PYGZus{}first\PYGZus{}yr\PYGZus{}avail}\PYG{l+s+s2}{\PYGZdq{}} \PYG{p}{:} \PYG{l+m+mi}{1956}\PYG{p}{,}
  \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{md\PYGZus{}last\PYGZus{}yr\PYGZus{}avail}\PYG{l+s+s2}{\PYGZdq{}} \PYG{p}{:} \PYG{l+m+mi}{2010}
\PYG{p}{\PYGZcb{}}
\end{sphinxVerbatim}

This file has its default configuration parameters in the outer keys.
Source\sphinxhyphen{}deck specific configuration can be applied by specifying a configuration
parameter under a \sphinxstyleemphasis{sid\sphinxhyphen{}dck} key. In the sample above, only the default
configuration is applied.


\subsection{Level 1e configuration file}
\label{\detokenize{index:level-1e-configuration-file}}\label{\detokenize{index:level1e-config-file}}
Create file \sphinxstyleemphasis{release\_config\_dir}/level1e.json.

The level1e specific parameters included in this file are:
\begin{itemize}
\item {}
“qc\_first\_date\_avail” : first monthly quality control file the process can
expect to find. If the data files to process are prior to this date, then
the data files will progress to the next level without quality flag merging
and without raising an error.

\item {}
“qc\_last\_date\_avail” : last monthly quality control file the process can
expect to find. If the data files to process are later to this date, then
the data files will progress to the next level without quality flag merging
and without raising an error.

\item {}
“history\_explain” : text added to the header file history field when flags are
merged.

\end{itemize}

The figure below shows a sample of this file:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{p}{\PYGZob{}}
  \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{job\PYGZus{}memo\PYGZus{}mb}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+m+mi}{4000}\PYG{p}{,}
  \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{job\PYGZus{}time\PYGZus{}hr}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{01}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
  \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{job\PYGZus{}time\PYGZus{}min}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{30}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
  \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{qc\PYGZus{}first\PYGZus{}date\PYGZus{}avail}\PYG{l+s+s2}{\PYGZdq{}} \PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{1850\PYGZhy{}02}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
  \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{qc\PYGZus{}last\PYGZus{}date\PYGZus{}avail}\PYG{l+s+s2}{\PYGZdq{}} \PYG{p}{:}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{2010\PYGZhy{}11}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
  \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{history\PYGZus{}explain}\PYG{l+s+s2}{\PYGZdq{}} \PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Position, tracking and parameter QC flags added}\PYG{l+s+s2}{\PYGZdq{}}
\PYG{p}{\PYGZcb{}}
\end{sphinxVerbatim}

This file has its default configuration parameters in the outer keys.
Source\sphinxhyphen{}deck specific configuration can be applied by specifying a configuration
parameter under a \sphinxstyleemphasis{sid\sphinxhyphen{}dck} key. In the sample above, only the default
configuration is applied.


\section{Set up the release data directory}
\label{\detokenize{index:set-up-the-release-data-directory}}
Every new release or new dataset in a release needs to have its corresponding
directory structure initialised in the file system:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{cd} obs\PYGZhy{}suite
\PYG{n+nb}{source} setpaths.sh
\PYG{n+nb}{source} setenv0.sh
\PYG{n+nb}{cd} scripts
python make\PYGZus{}release\PYGZus{}source\PYGZus{}tree.py \PYG{n+nv}{\PYGZdl{}data\PYGZus{}directory} \PYG{n+nv}{\PYGZdl{}config\PYGZus{}directory} release update dataset
\end{sphinxVerbatim}

where:
\begin{itemize}
\item {}
release: release identifier in file system

\item {}
update: release update identifier in file system

\item {}
dataset: dataset identifier in file system

\end{itemize}

This script does not overwrite existing directories and is safe to run on an
existing directory structure if new source\sphinxhyphen{}decks have to be added.


\chapter{Level 1a}
\label{\detokenize{index:level-1a}}
Level 1a contains the initial data converted from the input data sources
(level0) to files compatible with the CDM.

Every monthly file of the individual source\sphinxhyphen{}deck ICOADS dataset partitions is
converted with the following command:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{cd} obs\PYGZhy{}suite
\PYG{n+nb}{source} setpaths.sh
\PYG{n+nb}{source} setenv0.sh
\PYG{n+nb}{cd} scripts
python level1a.py \PYG{n+nv}{\PYGZdl{}data\PYGZus{}directory} release update dataset level1a\PYGZus{}config sid\PYGZhy{}dck year month
\end{sphinxVerbatim}

where:
\begin{itemize}
\item {}
release: release identifier in file system

\item {}
update: release update identifier in file system

\item {}
dataset: dataset identifier in file system

\item {}
level1a\_config: path to the level1a configuration file ( {\hyperref[\detokenize{index:level1a-config-file}]{\sphinxcrossref{\DUrole{std,std-ref}{Level 1a configuration file}}}})

\item {}
sid\sphinxhyphen{}dck: source\sphinxhyphen{}deck identifier

\item {}
year: file year, format yyyy

\item {}
month: file month, format mm

\end{itemize}

To facilitate the processing of a large number of files level1a.py can be run
in batch mode:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{cd} obs\PYGZhy{}suite
\PYG{n+nb}{source} setpaths.sh
\PYG{n+nb}{source} setenv0.sh
\PYG{n+nb}{cd} lotus\PYGZus{}scripts
python level1a\PYGZus{}slurm.py release update dataset \PYG{n+nv}{\PYGZdl{}config\PYGZus{}directory} process\PYGZus{}list \PYGZhy{}\PYGZhy{}failed\PYGZus{}only yes\PYG{p}{|}no
\end{sphinxVerbatim}

where:
\begin{itemize}
\item {}
release: release identifier in file system

\item {}
update: release update identifier in file system

\item {}
dataset: dataset identifier in file system

\item {}
process\_list: full path to file with the list of source\sphinxhyphen{}deck partitions to
process. This file can be either {\hyperref[\detokenize{index:process-list-file}]{\sphinxcrossref{\DUrole{std,std-ref}{Process list file}}}} or a subset of it.

\item {}
failed\_only: optional (yes|no). Defaults to no. Setting this argument to ‘yes’
means that only the monthly files with a *.failed log file will be processed.

\end{itemize}

This script executes an array of monthly subjobs per source and deck included in
the process\_list. The configuration for the process is directly accessed from
the release configuration directory: the data period processed is as configured
per source and deck in the release periods file ( {\hyperref[\detokenize{index:release-periods-file}]{\sphinxcrossref{\DUrole{std,std-ref}{Release periods file}}}})
and the level1a configuration is retrieved from {\hyperref[\detokenize{index:level1a-config-file}]{\sphinxcrossref{\DUrole{std,std-ref}{Level 1a configuration file}}}}.

This script logs to \sphinxstyleemphasis{data\_dir}/release/dataset/level1a/log/sid\sphinxhyphen{}dck/. Log files
are yyyy\sphinxhyphen{}mm\sphinxhyphen{}\textless{}release\textgreater{}\sphinxhyphen{}\textless{}update\textgreater{}.ext with ext either ok or failed depending on the
subjob termination status.

List  *.failed in the sid\sphinxhyphen{}dck level1a log directories to find if any went wrong.


\chapter{Level 1b}
\label{\detokenize{index:level-1b}}
Level 1b integrates external files containing enhanced information on the
duplicate status of the observations, corrected date/time and locations, and
linked station IDs with the level1a data. As part of the integration a weather
report may move between months if an error in the date had previously been
identified and the correct data is for a different month. A description of the
processing used to generate these external files is described in the marine
duplication identification document (available upon request,
to be published shortly). It should be noted that this processing is currently
external to C3S311a\_lot2 but will be integrated in a future release.

The external files need to be copied to the datasets directory in the marine
data directory prior to processing (datasets/NOC\_corrections/\sphinxstyleemphasis{cor\_version}).
Once copied to the required directory structure the files need to be reformatted
for integration with the level1a files. This is processing is done via python
and shell scripts using the SLURM scheduler. The following block needs to be run
once for each of the options id, datepos or duplicates:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{cd} obs\PYGZhy{}suite
\PYG{n+nb}{source} setpaths.sh
\PYG{n+nb}{source} setenv0.sh
\PYG{n+nb}{cd} scripts
\PYG{n+nv}{option}\PYG{o}{=}option
sbatch \PYGZhy{}J \PYG{n+nv}{\PYGZdl{}option} \PYGZhy{}o \PYG{n+nv}{\PYGZdl{}option}.out \PYGZhy{}e \PYG{n+nv}{\PYGZdl{}option}.out \PYGZhy{}p short\PYGZhy{}serial \PYGZhy{}t \PYG{l+m}{03}:00:00 \PYGZhy{}\PYGZhy{}mem \PYG{l+m}{1000} \PYGZhy{}\PYGZhy{}open\PYGZhy{}mode truncate \PYGZhy{}\PYGZhy{}wrap\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{./noc\PYGZus{}corrections\PYGZus{}postprocess.sh release cor\PYGZus{}version }\PYG{n+nv}{\PYGZdl{}option}\PYG{l+s+s2}{ year\PYGZus{}init year\PYGZus{}end}\PYG{l+s+s2}{\PYGZdq{}}
\end{sphinxVerbatim}

where:
\begin{itemize}
\item {}
release: release tag

\item {}
option: id, datepos or duplicates

\item {}
cor\_version: NOC correction version (v1x2019 for release 1 and release 2)

\item {}
year\_init|end: first|last year of data release.

\end{itemize}

This step places the reformatted correction files in the release directory in
the marine data directory (\sphinxstyleemphasis{release}/NOC\_corrections/\sphinxstyleemphasis{cor\_version}) ready to be
merged with the CDM data files. The time needed will depend on the period
pre\sphinxhyphen{}processed. However if the job terminates due to insufficient time allocation,
the period remaining to be pre\sphinxhyphen{}processed can be launched independently, and it
will not affect the files already processed.

The reformatted files are merged with the level1a data by the following command:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{cd} obs\PYGZhy{}suite
\PYG{n+nb}{source} setpaths.sh
\PYG{n+nb}{source} setenv0.sh
\PYG{n+nb}{cd} scripts
python level1b.py \PYG{n+nv}{\PYGZdl{}data\PYGZus{}directory} release update dataset level1b\PYGZus{}config sid\PYGZhy{}dck year month
\end{sphinxVerbatim}

where:
\begin{itemize}
\item {}
release: release identifier in file system

\item {}
update: release update identifier in file system

\item {}
dataset: dataset identifier in file system

\item {}
level1b\_config: path to the level1b configuration file ( {\hyperref[\detokenize{index:level1b-config-file}]{\sphinxcrossref{\DUrole{std,std-ref}{Level 1b configuration file}}}})

\item {}
sid\sphinxhyphen{}dck: source\sphinxhyphen{}deck identifier

\item {}
year: file year, format yyyy

\item {}
month: file month, format mm

\end{itemize}

To facilitate the processing of a large number of files level1b.py can be run
in batch mode:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{cd} obs\PYGZhy{}suite
\PYG{n+nb}{source} setpaths.sh
\PYG{n+nb}{source} setenv0.sh
\PYG{n+nb}{cd} lotus\PYGZus{}scripts
python level1b\PYGZus{}slurm.py release update dataset \PYG{n+nv}{\PYGZdl{}config\PYGZus{}directory} process\PYGZus{}list \PYGZhy{}\PYGZhy{}failed\PYGZus{}only yes\PYG{p}{|}no
\end{sphinxVerbatim}

where:
\begin{itemize}
\item {}
release: release identifier in file system

\item {}
update: release update identifier in file system

\item {}
dataset: dataset identifier in file system

\item {}
process\_list: full path to file with the list of source\sphinxhyphen{}deck partitions to
process. This file can be either {\hyperref[\detokenize{index:process-list-file}]{\sphinxcrossref{\DUrole{std,std-ref}{Process list file}}}} or a subset of it.

\item {}
failed\_only: optional (yes|no). Defaults to no. Setting this argument to ‘yes’
means that only the monthly files with a *.failed log file will be processed.

\end{itemize}

This script executes an array of monthly subjobs per source and deck included in
the process\_list. The configuration for the process is directly accessed from
the release configuration directory: the data period processed is as configured
per source and deck in the release periods file ( {\hyperref[\detokenize{index:release-periods-file}]{\sphinxcrossref{\DUrole{std,std-ref}{Release periods file}}}})
and the level1b configuration is retrieved from {\hyperref[\detokenize{index:level1b-config-file}]{\sphinxcrossref{\DUrole{std,std-ref}{Level 1b configuration file}}}}.

This script logs to \sphinxstyleemphasis{data\_dir}/release/dataset/level1b/log/sid\sphinxhyphen{}dck/. Log files
are yyyy\sphinxhyphen{}mm\sphinxhyphen{}\textless{}release\textgreater{}\sphinxhyphen{}\textless{}update\textgreater{}.ext with ext either ok or failed depending on the
subjob termination status.

List  *.failed in the sid\sphinxhyphen{}dck level1b log directories to find if any went wrong.


\chapter{Level 1c}
\label{\detokenize{index:level-1c}}
Level1c files contain reports from level1b that have been further validated
following corrections to the date/time, location and station ID. Those failing
validation are rejected and archived for future analysis. Additionally, datetime
corrections applied previously in level1b, can potentially result in reports
being relocated to a different month. These reports are moved to their correct
monthly file in this level.

To generate level1c files, the individual sid\sphinxhyphen{}dck monthly files in level1b are
processed with:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{cd} obs\PYGZhy{}suite
\PYG{n+nb}{source} setpaths.sh
\PYG{n+nb}{source} setenv0.sh
\PYG{n+nb}{cd} scripts
python level1c.py \PYG{n+nv}{\PYGZdl{}data\PYGZus{}directory} release update dataset level1c\PYGZus{}config sid\PYGZhy{}dck year month
\end{sphinxVerbatim}

where:
\begin{itemize}
\item {}
release: release identifier in file system

\item {}
update: release update identifier in file system

\item {}
dataset: dataset identifier in file system

\item {}
level1c\_config: path to the level1c configuration file ( {\hyperref[\detokenize{index:level1c-config-file}]{\sphinxcrossref{\DUrole{std,std-ref}{Level 1c configuration file}}}})

\item {}
sid\sphinxhyphen{}dck: source\sphinxhyphen{}deck identifier

\item {}
year: file year, format yyyy

\item {}
month: file month, format mm

\end{itemize}

To facilitate the processing of a large number of files level1c.py can be run
in batch mode:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{cd} obs\PYGZhy{}suite
\PYG{n+nb}{source} setpaths.sh
\PYG{n+nb}{source} setenv0.sh
\PYG{n+nb}{cd} lotus\PYGZus{}scripts
python level1c\PYGZus{}slurm.py release update dataset \PYG{n+nv}{\PYGZdl{}config\PYGZus{}directory} process\PYGZus{}list \PYGZhy{}\PYGZhy{}failed\PYGZus{}only yes\PYG{p}{|}no \PYGZhy{}\PYGZhy{}remove\PYGZus{}source yes\PYG{p}{|}no
\end{sphinxVerbatim}

where:
\begin{itemize}
\item {}
release: release identifier in file system

\item {}
update: release update identifier in file system

\item {}
dataset: dataset identifier in file system

\item {}
process\_list: full path to file with the list of source\sphinxhyphen{}deck partitions to
process. This file can be either {\hyperref[\detokenize{index:process-list-file}]{\sphinxcrossref{\DUrole{std,std-ref}{Process list file}}}} or a subset of it.

\item {}
failed\_only: optional (yes|no). Defaults to no. Setting this argument to ‘yes’
means that only the monthly files with a *.failed log file will be processed.

\item {}
remove\_source: optional (yes|no). Defaults to no. Setting this argument to ‘yes’
implies removal of the source level (level1b) data files if the full set of
monthly data files of a given source\sphinxhyphen{}deck is successfully processed.

\end{itemize}

This script executes an array of monthly subjobs per source and deck included in
the process\_list. The configuration for the process is directly accessed from
the release configuration directory: the data period processed is as configured
per source and deck in the release periods file ( {\hyperref[\detokenize{index:release-periods-file}]{\sphinxcrossref{\DUrole{std,std-ref}{Release periods file}}}})
and the level1c configuration is retrieved from {\hyperref[\detokenize{index:level1c-config-file}]{\sphinxcrossref{\DUrole{std,std-ref}{Level 1c configuration file}}}}.

This script logs to \sphinxstyleemphasis{data\_dir}/release/dataset/level1c/log/sid\sphinxhyphen{}dck/. Log files
are yyyy\sphinxhyphen{}mm\sphinxhyphen{}\textless{}release\textgreater{}\sphinxhyphen{}\textless{}update\textgreater{}.ext with ext either ok or failed depending on the
subjob termination status.

List  *.failed in the sid\sphinxhyphen{}dck level1c log directories to find if any went wrong.


\chapter{Level 1d}
\label{\detokenize{index:level-1d}}
The level1d files contain the level1c data merged with external metadata (where
available). In the current marine processing implementation, the level1c are
merged with WMO Publication 47 metadata to set instrument heights, station names
and platform sub types (i.e. type of ship). Prior to merging, the WMO
Publication 47 metadata are harmonised, quality controlled and pre\sphinxhyphen{}processed in
a process that run independently to this data flow (add ref). After
pre\sphinxhyphen{}processing, this info needs to be made available to the release in directory
\sphinxstyleemphasis{data\_directory}/\sphinxstyleemphasis{release}/wmo\_publication\_47/monthly/.

To generate level1d files, the individual sid\sphinxhyphen{}dck monthly files in level1c are
processed with:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{cd} obs\PYGZhy{}suite
\PYG{n+nb}{source} setpaths.sh
\PYG{n+nb}{source} setenv0.sh
\PYG{n+nb}{cd} scripts
python level1d.py \PYG{n+nv}{\PYGZdl{}data\PYGZus{}directory} release update dataset level1d\PYGZus{}config sid\PYGZhy{}dck year month
\end{sphinxVerbatim}

where:
\begin{itemize}
\item {}
release: release identifier in file system

\item {}
update: release update identifier in file system

\item {}
dataset: dataset identifier in file system

\item {}
level1d\_config: path to the level1d configuration file ( {\hyperref[\detokenize{index:level1d-config-file}]{\sphinxcrossref{\DUrole{std,std-ref}{Level 1d configuration file}}}})

\item {}
sid\sphinxhyphen{}dck: source\sphinxhyphen{}deck identifier

\item {}
year: file year, format yyyy

\item {}
month: file month, format mm

\end{itemize}

To facilitate the processing of a large number of files level1d.py can be run
in batch mode:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{cd} obs\PYGZhy{}suite
\PYG{n+nb}{source} setpaths.sh
\PYG{n+nb}{source} setenv0.sh
\PYG{n+nb}{cd} lotus\PYGZus{}scripts
python level1d\PYGZus{}slurm.py release update dataset \PYG{n+nv}{\PYGZdl{}config\PYGZus{}directory} process\PYGZus{}list \PYGZhy{}\PYGZhy{}failed\PYGZus{}only yes\PYG{p}{|}no \PYGZhy{}\PYGZhy{}remove\PYGZus{}source yes\PYG{p}{|}no
\end{sphinxVerbatim}

where:
\begin{itemize}
\item {}
release: release identifier in file system

\item {}
update: release update identifier in file system

\item {}
dataset: dataset identifier in file system

\item {}
process\_list: full path to file with the list of source\sphinxhyphen{}deck partitions to
process. This file can be either {\hyperref[\detokenize{index:process-list-file}]{\sphinxcrossref{\DUrole{std,std-ref}{Process list file}}}} or a subset of it.

\item {}
failed\_only: optional (yes|no). Defaults to no. Setting this argument to ‘yes’
means that only the monthly files with a *.failed log file will be processed.

\item {}
remove\_source: optional (yes|no). Defaults to no. Setting this argument to ‘yes’
implies removal of the source level (level1c) data files if the full set of
monthly data files of a given source\sphinxhyphen{}deck is successfully processed.

\end{itemize}

This script executes an array of monthly subjobs per source and deck included in
the process\_list. The configuration for the process is directly accessed from
the release configuration directory: the data period processed is as configured
per source and deck in the release periods file ( {\hyperref[\detokenize{index:release-periods-file}]{\sphinxcrossref{\DUrole{std,std-ref}{Release periods file}}}})
and the level1d configuration is retrieved from {\hyperref[\detokenize{index:level1d-config-file}]{\sphinxcrossref{\DUrole{std,std-ref}{Level 1d configuration file}}}}.

This script logs to \sphinxstyleemphasis{data\_dir}/release/dataset/level1d/log/sid\sphinxhyphen{}dck/. Log files
are yyyy\sphinxhyphen{}mm\sphinxhyphen{}\textless{}release\textgreater{}\sphinxhyphen{}\textless{}update\textgreater{}.ext with ext either ok or failed depending on the
subjob termination status.

List  *.failed in the sid\sphinxhyphen{}dck level1d log directories to find if any went wrong.


\chapter{Level 1e}
\label{\detokenize{index:level-1e}}
The level1e processing merges the data quality flags from the Met Office QC
suite (add ref) with the data from level 1d. The QC software generates two sets
of QC files, one basic QC of the observations from all platforms and an enhanced
track and quality check for drifting buoy data. The basic QC flags are stored in
\sphinxstyleemphasis{data\_directory}/\sphinxstyleemphasis{release}/\sphinxstyleemphasis{dataset}/metoffice\_qc/base/ and merged with the
following script:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{cd} obs\PYGZhy{}suite
\PYG{n+nb}{source} setpaths.sh
\PYG{n+nb}{source} setenv0.sh
\PYG{n+nb}{cd} scripts
python level1e.py \PYG{n+nv}{\PYGZdl{}data\PYGZus{}directory} release update dataset level1e\PYGZus{}config sid\PYGZhy{}dck year month
\end{sphinxVerbatim}

where:
\begin{itemize}
\item {}
release: release identifier in file system

\item {}
update: release update identifier in file system

\item {}
dataset: dataset identifier in file system

\item {}
level1e\_config: path to the level1e configuration file ( {\hyperref[\detokenize{index:level1e-config-file}]{\sphinxcrossref{\DUrole{std,std-ref}{Level 1e configuration file}}}})

\item {}
sid\sphinxhyphen{}dck: source\sphinxhyphen{}deck identifier

\item {}
year: file year, format yyyy

\item {}
month: file month, format mm

\end{itemize}

To facilitate the processing of a large number of files level1e.py can be run
in batch mode:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{cd} obs\PYGZhy{}suite
\PYG{n+nb}{source} setpaths.sh
\PYG{n+nb}{source} setenv0.sh
\PYG{n+nb}{cd} lotus\PYGZus{}scripts
python level1e\PYGZus{}slurm.py release update dataset \PYG{n+nv}{\PYGZdl{}config\PYGZus{}directory} process\PYGZus{}list \PYGZhy{}\PYGZhy{}failed\PYGZus{}only yes\PYG{p}{|}no \PYGZhy{}\PYGZhy{}remove\PYGZus{}source yes\PYG{p}{|}no
\end{sphinxVerbatim}

where:
\begin{itemize}
\item {}
release: release identifier in file system

\item {}
update: release update identifier in file system

\item {}
dataset: dataset identifier in file system

\item {}
process\_list: full path to file with the list of source\sphinxhyphen{}deck partitions to
process. This file can be either {\hyperref[\detokenize{index:process-list-file}]{\sphinxcrossref{\DUrole{std,std-ref}{Process list file}}}} or a subset of it.

\item {}
failed\_only: optional (yes|no). Defaults to no. Setting this argument to ‘yes’
means that only the monthly files with a *.failed log file will be processed.

\item {}
remove\_source: optional (yes|no). Defaults to no. Setting this argument to ‘yes’
implies removal of the source level (level1d) data files if the full set of
monthly data files of a given source\sphinxhyphen{}deck is successfully processed.

\end{itemize}

This script executes an array of monthly subjobs per source and deck included in
the process\_list. The configuration for the process is directly accessed from
the release configuration directory: the data period processed is as configured
per source and deck in the release periods file ( {\hyperref[\detokenize{index:release-periods-file}]{\sphinxcrossref{\DUrole{std,std-ref}{Release periods file}}}})
and the level1d configuration is retrieved from {\hyperref[\detokenize{index:level1e-config-file}]{\sphinxcrossref{\DUrole{std,std-ref}{Level 1e configuration file}}}}.

This script logs to \sphinxstyleemphasis{data\_dir}/release/dataset/level1e/log/sid\sphinxhyphen{}dck/. Log files
are yyyy\sphinxhyphen{}mm\sphinxhyphen{}\textless{}release\textgreater{}\sphinxhyphen{}\textless{}update\textgreater{}.ext with ext either ok or failed depending on the
subjob termination status.

List  *.failed in the sid\sphinxhyphen{}dck level1e log directories to find if any went wrong.

After the basic QC flags have been merged the enhanced drifting buoy flags need
to be merged with the level1e data. This process is described under Quality
control in 4.4.6 in the C3S Technical Service Document (but will be moved to the level1e
processing in a future update).

Once the drifting buoy flags have been merged the data files will no longer
change and summary data reports need to be generated prior to the data moving to
level2. See documentation in \sphinxhref{mailto:git@git.noc.ac.uk}{git@git.noc.ac.uk}:iregon/marine\sphinxhyphen{}user\sphinxhyphen{}guide.git to
create the reports.


\chapter{Level 2}
\label{\detokenize{index:level-2}}
After visual inspection of the reports generated in level1e, only observation
tables reaching a minimum quality standard proceed to level2: this might imply
rejecting a full sid\sphinxhyphen{}dck dataset or an observational table or change the period
of data to release. The level1e data composition that has been used to generate
the level2 product of every release is configured in level2.json file available
in the release configuration directory. Prior to first use this file needs to be
created. This can be done using the following commands:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{cd} obs\PYGZhy{}suite
\PYG{n+nb}{source} setpaths.sh
\PYG{n+nb}{source} setenv0.sh
\PYG{n+nb}{cd} scripts
python level2\PYGZus{}config.py release\PYGZus{}period\PYGZus{}file year\PYGZus{}ini year\PYGZus{}end
\end{sphinxVerbatim}

where:
\begin{itemize}
\item {}
release\_periods\_file: full path to the release periods file ( {\hyperref[\detokenize{index:release-periods-file}]{\sphinxcrossref{\DUrole{std,std-ref}{Release periods file}}}} )

\item {}
year\_ini: first year in data release period.

\item {}
year\_end: last year in data release period.

\end{itemize}

This script creates the selection file level2.json in the execution directory.
The parameters year\_init and year\_end are used to set the final period of data
release, that might be different to that initially processed. The data period of
each of the individual source\sphinxhyphen{}deck data partitions is adjusted in the level2.json
file according to these arguments.

After checking data quality on the level1e
reports, edit the data selection file as needed to create the final dataset
composition:
\begin{itemize}
\item {}
Remove a full sid\sphinxhyphen{}dck from level2 by setting to true the sid\sphinxhyphen{}dck ‘exclude’ tag.

\item {}
Remove an observation table from the full dataset by adding it to the list under the general ‘params\_exclude’ tag.

\item {}
Remove an observation table from a sid\sphinxhyphen{}dck by adding it to the list under the sid\sphinxhyphen{}dck ‘params\_exclude’ tag.

\item {}
Adjust the release period of a sid\sphinxhyphen{}dck by modifying the ‘year\_init|end’ tags of the sid\sphinxhyphen{}dck

\item {}
Observation tables to be removed have to be named as observations\sphinxhyphen{}{[}at|sst|dpt|wbt|wd|ws|slp{]}

\item {}
All edits need to be consistent with JSON formatting rules.

\end{itemize}

Once file level2.json has been edited the file needs to be copied to its
directory in the configuration repository to be version controlled.

In the file sample below, all observations\sphinxhyphen{}wbt table files will be retained for
level2, while observations\sphinxhyphen{}at will be dropped from level2 only in source\sphinxhyphen{}deck
063\sphinxhyphen{}714.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{p}{\PYGZob{}}
    \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{001\PYGZhy{}110}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{p}{\PYGZob{}}
        \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{year\PYGZus{}init}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+m+mi}{1948}\PYG{p}{,}
        \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{year\PYGZus{}end}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+m+mi}{1951}\PYG{p}{,}
        \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{exclude}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{n}{false}\PYG{p}{,}
        \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{params\PYGZus{}exclude}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{p}{[}\PYG{p}{]}
    \PYG{p}{\PYGZcb{}}\PYG{p}{,}
    \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{063\PYGZhy{}714}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{p}{\PYGZob{}}
        \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{year\PYGZus{}init}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+m+mi}{2007}\PYG{p}{,}
        \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{year\PYGZus{}end}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+m+mi}{2009}\PYG{p}{,}
        \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{exclude}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{n}{false}\PYG{p}{,}
        \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{params\PYGZus{}exclude}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{observations\PYGZhy{}at}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}
    \PYG{p}{\PYGZcb{}}\PYG{p}{,}
    \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{171\PYGZhy{}711}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{p}{\PYGZob{}}
        \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{year\PYGZus{}init}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+m+mi}{1889}\PYG{p}{,}
        \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{year\PYGZus{}end}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+m+mi}{1899}\PYG{p}{,}
        \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{exclude}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{n}{false}\PYG{p}{,}
        \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{params\PYGZus{}exclude}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{p}{[}\PYG{p}{]}
    \PYG{p}{\PYGZcb{}}\PYG{p}{,}
    \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{params\PYGZus{}exclude}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{observations\PYGZhy{}wbt}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{,}
    \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{year\PYGZus{}init}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+m+mi}{1889}\PYG{p}{,}
    \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{year\PYGZus{}end}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+m+mi}{2010}
\PYG{p}{\PYGZcb{}}
\end{sphinxVerbatim}

The level 2 processing is then run using:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{cd} obs\PYGZhy{}suite
\PYG{n+nb}{source} setpaths.sh
\PYG{n+nb}{source} setenv0.sh
\PYG{n+nb}{cd} scripts
python level2.py data\PYGZus{}directory release update dataset level2\PYGZus{}config sid\PYGZhy{}dck
\end{sphinxVerbatim}

where:
\begin{itemize}
\item {}
release: release identifier in file system

\item {}
update: release update identifier in file system

\item {}
dataset: dataset identifier in file system

\item {}
level2\_config: path to the level2 configuration file

\item {}
sid\sphinxhyphen{}dck: source\sphinxhyphen{}deck identifier

\end{itemize}

The launcher script for level2 is run with:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{cd} obs\PYGZhy{}suite
\PYG{n+nb}{source} setpaths.sh
\PYG{n+nb}{source} setenv0.sh
\PYG{n+nb}{cd} lotus\PYGZus{}scripts
python level2\PYGZus{}slurm.py release update dataset \PYG{n+nv}{\PYGZdl{}config\PYGZus{}directory} process\PYGZus{}list
\end{sphinxVerbatim}

where:
\begin{itemize}
\item {}
release: release identifier in file system

\item {}
update: release update identifier in file system

\item {}
dataset: dataset identifier in file system

\item {}
process\_list: full path to file with the list of source\sphinxhyphen{}deck partitions to
process. This file can be either {\hyperref[\detokenize{index:process-list-file}]{\sphinxcrossref{\DUrole{std,std-ref}{Process list file}}}} or a subset of it.

\end{itemize}

This script executes a job per source and deck included in the process\_list.
The configuration file for the process (level2.json) is directly accessed from
the release configuration directory.

This script logs to \sphinxstyleemphasis{data\_dir}/release/dataset/level2/log/sid\sphinxhyphen{}dck/. Log files
are \sphinxstyleemphasis{sid\sphinxhyphen{}dck}\sphinxhyphen{}\sphinxstyleemphasis{release}\sphinxhyphen{}\sphinxstyleemphasis{update}.ext with ext either ok or failed depending on the
subjob termination status.

List  *.failed in the sid\sphinxhyphen{}dck level2 log directories to find if any went wrong.



\renewcommand{\indexname}{Index}
\printindex
\end{document}
